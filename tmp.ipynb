{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "from models import *\n",
    "from dataprocessor import *\n",
    "def reload_custom_libs():\n",
    "    import loss_functions\n",
    "    import models\n",
    "    import dataprocessor\n",
    "    importlib.reload(loss_functions)\n",
    "    importlib.reload(models)\n",
    "    importlib.reload(dataprocessor)\n",
    "    \n",
    "folder_path = 'data'\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "files = [file for file in files if file not in (\"CC00.NYB.xlsx\", \"LB00.CME.xlsx\", \"ES00.CME.xlsx\", \"NQ00.CME.xlsx\", \"YM00.CBT.xlsx\", \"SP00.CME.xlsx\")]\n",
    "macd_timescales = [(8, 24), (16, 28), (32, 96)]\n",
    "rtn_timescales = [1, 21, 63, 126, 252]\n",
    "timesteps = 126"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理文件中。。: 100%|██████████| 11/11 [01:02<00:00,  5.65s/it]\n",
      "生成张量, 并对类别信息进行on-hot 编码: 100%|██████████| 11/11 [00:00<00:00, 17.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot 编码中...\n",
      "123456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成张量, 并对类别信息进行on-hot 编码: 100%|██████████| 11/11 [00:00<00:00, 14.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot 编码中...\n",
      "123456\n"
     ]
    }
   ],
   "source": [
    "data_list = process_data_list(files, macd_timescales, rtn_timescales, test = True)\n",
    "target_set, labels, map = generate_tensors(data_list, timesteps, encoder_type = \"one-hot\", return_map=True)\n",
    "context_set, _ = generate_tensors(data_list, timesteps, encoder_type = \"one-hot\", contain_next_day_rtn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(files)\n",
    "# data_set = data_binder(context_set, target_set, labels, batch_size=batch_size)\n",
    "# data_set.save(f\"saved_data\")\n",
    "\n",
    "dataset = tf.data.Dataset.load(\"saved_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 126, 64)\n"
     ]
    }
   ],
   "source": [
    "reload_custom_libs()\n",
    "from models import *\n",
    "from dataprocessor import *\n",
    "from loss_functions import *\n",
    "\n",
    "hidden_dim = 64 # 128\n",
    "warm_up = 63\n",
    "target_std = tf.cast(5e-2, tf.float64)\n",
    "features_len = len(macd_timescales) + len(rtn_timescales)\n",
    "encoding_size = len(files) + 1\n",
    "x_shape = (batch_size, timesteps, features_len)\n",
    "s_shape = (batch_size, timesteps, encoding_size)\n",
    "\n",
    "model = ModelWrapper(features_len, hidden_dim, encoding_size, num_heads = 4)\n",
    "model.build((x_shape, s_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(batch_num: int, num_epochs: int):\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    dataset = tf.data.Dataset.load(\"saved_data\").shuffle(buffer_size=10000).batch(batch_num)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        iter_count = 0\n",
    "        for batch_data in tqdm(dataset, desc=f\"训练中...epoch{epoch}\"):\n",
    "            # 对每一个批次进行处理\n",
    "            x_c_b, x_c_rtn_b, s_c_b, x_b, s_b, rtn_std_b, _, _ = batch_data\n",
    "            with tf.GradientTape() as tape:\n",
    "                losses = []\n",
    "                mles = []\n",
    "                sharpes = []\n",
    "                # 遍历批次中的每一个样本\n",
    "                for i in range(batch_num):\n",
    "                    x_c, x_c_rtn, s_c, x, s, rtn_std = x_c_b[i], x_c_rtn_b[i], s_c_b[i], x_b[i], s_b[i], rtn_std_b[i]\n",
    "                    result = model(x_c, x_c_rtn, s_c, x, s)\n",
    "                    loss, mle, sharpe = joint_loss_function(result, rtn_std, target_std, warm_up, alpha=1)\n",
    "                    losses.append(loss)\n",
    "                    mles.append(mle)\n",
    "                    sharpes.append(sharpe)\n",
    "\n",
    "                # 计算批次的平均损失\n",
    "                batch_loss = tf.reduce_mean(losses)\n",
    "                batch_mle = tf.reduce_mean(mles)\n",
    "                batch_sharpe = tf.reduce_mean(sharpes)\n",
    "\n",
    "            grads = tape.gradient(batch_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "            iter_count += 1\n",
    "            if iter_count % 1 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Iteration {iter_count}, Loss: {batch_loss.numpy()}, MLE: {batch_mle.numpy()}, Sharpe: {batch_sharpe.numpy()}\")\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {batch_loss.numpy()}\")\n",
    "\n",
    "result = train(batch_num = 126, num_epochs = 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtrend-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
