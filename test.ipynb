{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, ELU, Add, Softmax, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN_j(tf.keras.Model):\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"初始化 VSN 中针对单个事件点每个特征的FFN\n",
    "\n",
    "        Args:\n",
    "            hidden_dim (int): 隐藏层维度\n",
    "        \"\"\"\n",
    "        super(FFN_j, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.linear_1 = Dense(units=hidden_dim)\n",
    "        self.elu = ELU()\n",
    "        self.linear_3 = Dense(units=hidden_dim)\n",
    "    \n",
    "    def call(self, h_t) -> tf.Tensor:\n",
    "        \"\"\"前向传播方法。\n",
    "\n",
    "        Args:\n",
    "            h_t (tf.Tensor): 输入的隐藏状态\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: 输出张量的维度，与隐藏层相等\n",
    "        \"\"\"\n",
    "        add_output = self.linear_1(h_t)\n",
    "        elu_output = self.elu(add_output)\n",
    "        output = self.linear_3(elu_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class FFN(tf.keras.Model):\n",
    "    def __init__(self, sequence_length, hidden_dim, encoding_size, output_dim = None):\n",
    "        \"\"\"初始化 Feed Forward network\n",
    "\n",
    "        Args:\n",
    "            sequence_length (int): 输入的序列长度\n",
    "            encoding_size (int): side info encoding的长度\n",
    "            hidden_dim (int): 隐藏层的维度\n",
    "        \"\"\"\n",
    "        super(FFN, self).__init__()\n",
    "        if output_dim is None: output_dim = sequence_length\n",
    "        self.sequence_length = sequence_length\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoding_size = encoding_size\n",
    "        \n",
    "        self.linear_1 = Dense(units=hidden_dim)\n",
    "        self.elu = ELU()\n",
    "        self.linear_2 = Dense(units=hidden_dim)       \n",
    "        self.linear_3 = Dense(units=output_dim)\n",
    "    \n",
    "    def call(self, h_t, s) -> tf.Tensor:\n",
    "        \"\"\"前向传播方法。\n",
    "\n",
    "        Args:\n",
    "            h_t (tf.Tensor): 输入的隐藏状态\n",
    "            s (tf.Tensor, optional): side information 的输入张量\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: 输出张量的维度与输入的序列长度相等\n",
    "        \"\"\"\n",
    "        linear_1_output = self.linear_1(h_t)\n",
    "        linear_2_output = self.linear_2(s)\n",
    "        add_output = linear_1_output + linear_2_output\n",
    "        elu_output = self.elu(add_output)\n",
    "        output = self.linear_3(elu_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class VSN(tf.keras.Model):\n",
    "    def __init__(self, sequence_length, hidden_dim, encoding_size=None):\n",
    "        \"\"\"初始化加权求和网络 (VSN)。\n",
    "\n",
    "        Args:\n",
    "            sequence_length (int): 输入的序列长度\n",
    "            encoding_size (int): side info encoding的长度\n",
    "            hidden_dim (int): 隐藏层的维度，与输出张量维度相等\n",
    "        \"\"\"\n",
    "        super(VSN, self).__init__()\n",
    "        self.ffn = FFN(sequence_length, hidden_dim, encoding_size)\n",
    "        self.softmax = Softmax(axis=2)\n",
    "        self.sequence_FFN = [FFN_j(hidden_dim = hidden_dim) for _ in range(sequence_length)]\n",
    "        \n",
    "    def call(self, x_t, s=None):\n",
    "        \"\"\"前向传播方法。\n",
    "\n",
    "        Args:\n",
    "            x_t  (tf.Tensor): 输入的样本，形状为 [timestemps, N]\n",
    "            s (tf.Tensor, optional): side information 的输入张量\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: 输出张量的维度与隐藏层相等\n",
    "        \"\"\"\n",
    "        batch_size, time_steps, seq_len = x_t.shape\n",
    "        ffn_output = self.ffn(x_t, s)\n",
    "        w_t = self.softmax(ffn_output)\n",
    "        x_t_reshaped = tf.reshape(x_t, (-1, seq_len))\n",
    "        outputs = []\n",
    "        for i in range(self.ffn.sequence_length):\n",
    "            ff_output = self.sequence_FFN[i](tf.expand_dims(x_t_reshaped[:, i], axis=1))\n",
    "            outputs.append(ff_output)\n",
    "        \n",
    "        outputs = tf.stack(outputs, axis=1)  # Shape: [batch_size * time_steps, sequence_length, hidden_dim]\n",
    "        outputs = tf.reshape(outputs, (batch_size, time_steps, seq_len, -1))  # Reshape back\n",
    "\n",
    "        w_t_expanded = tf.expand_dims(w_t, axis=-1)  # Shape: [batch_size, time_steps, sequence_length, 1]\n",
    "        weighted_outputs = outputs * w_t_expanded  # Element-wise multiplication\n",
    "        vsn_output = tf.reduce_sum(weighted_outputs, axis=2)  # Sum over the sequence_length dimension\n",
    "        return vsn_output\n",
    "    \n",
    "    \n",
    "class BaselineNeuralForecaster(tf.keras.Model):\n",
    "    def __init__(self, sequence_length, hidden_dim, encoding_size):\n",
    "        super(BaselineNeuralForecaster, self).__init__()\n",
    "        self.vsn_model = VSN(sequence_length, hidden_dim, encoding_size=encoding_size)\n",
    "        self.FFN_3, self.FFN_4 = FFN_j(hidden_dim), FFN_j(hidden_dim)\n",
    "        self.lstm_model = tf.keras.layers.LSTM(hidden_dim)\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.lstm_model = tf.keras.layers.LSTM(hidden_dim)\n",
    "        self.lstm_model = tf.keras.layers.LSTM(hidden_dim)\n",
    "        self.FFN_2 = FFN(sequence_length, hidden_dim, encoding_size, output_dim=hidden_dim)\n",
    "    \n",
    "    \n",
    "    def call(self, x, s) -> tf.Tensor:\n",
    "        x_ = self.vsn_model(x, s)\n",
    "        \n",
    "        # Compute initial LSTM states\n",
    "        h_0, c_0 = self.FFN_3(s), self.FFN_4(s)\n",
    "        h_0, c_0 = tf.reduce_mean(h_0, axis=1), tf.reduce_mean(c_0, axis=1)\n",
    "        outputs = self.lstm_model(x_, initial_state=[h_0, c_0])\n",
    "        \n",
    "        # Compute the final output\n",
    "        a_t = LayerNormalization()(x_[:, -1, :] + outputs)\n",
    "        result = LayerNormalization()(self.FFN_2(a_t, s[:, 0, :]) + a_t)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\keras\\src\\layers\\layer.py:361: UserWarning: `build()` was called on layer 'baseline_neural_forecaster_2', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "sequence_length = 4\n",
    "hidden_dim = 8\n",
    "encoding_size = 17  # 17 种合约\n",
    "time_steps = 2  # 每个样本的时间步\n",
    "\n",
    "x = tf.random.uniform((batch_size, time_steps, sequence_length))\n",
    "s_indices = tf.tile(tf.range(batch_size)[:, tf.newaxis], [1, time_steps])\n",
    "s = tf.one_hot(s_indices + 6, depth=encoding_size)\n",
    "\n",
    "model = BaselineNeuralForecaster(sequence_length, hidden_dim, encoding_size)\n",
    "result = model(x, s)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "sequence_length = 4\n",
    "hidden_dim = 8\n",
    "encoding_size = 17  # 17 种合约\n",
    "time_steps = 2  # 每个样本的时间步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "sequence_length = 4\n",
    "hidden_dim = 8\n",
    "encoding_size = 17  # 17 种合约\n",
    "time_steps = 2  # 每个样本的时间步\n",
    "\n",
    "x = tf.random.uniform((batch_size, time_steps, sequence_length))  # 示例输入 h_t\n",
    "s_indices = tf.tile(tf.range(batch_size)[:, tf.newaxis], [1, time_steps])\n",
    "s = tf.one_hot(s_indices + 6, depth=encoding_size) \n",
    "\n",
    "vsn_model = VSN(sequence_length, hidden_dim, encoding_size=encoding_size)\n",
    "x_ = vsn_model(x, s)\n",
    "\n",
    "FFN_3, FFN_4 = FFN_j(hidden_dim), FFN_j(hidden_dim)\n",
    "h_0 = FFN_3(s)\n",
    "c_0 = FFN_4(s)\n",
    "h_0 = tf.reduce_mean(h_0, axis=1)  # Shape: (batch_size, hidden_dim)\n",
    "c_0 = tf.reduce_mean(c_0, axis=1)\n",
    "\n",
    "lstm_model = tf.keras.layers.LSTM(hidden_dim)\n",
    "outputs = lstm_model(x_, initial_state=[h_0, c_0])\n",
    "a_t = LayerNormalization()(x_[:, -1, :] + outputs)\n",
    "\n",
    "FFN_2 = FFN(sequence_length, hidden_dim, encoding_size, output_dim=hidden_dim)\n",
    "result = LayerNormalization()(FFN_2(a_t, s[:, 0, :]) + a_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling VSN.call().\n\n\u001b[1mnot enough values to unpack (expected 3, got 2)\u001b[0m\n\nArguments received by VSN.call():\n  • x_t=tf.Tensor(shape=(2, 4), dtype=float32)\n  • s=tf.Tensor(shape=(2, 17), dtype=float32)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m s \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mone_hot(s_indices \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m6\u001b[39m, depth\u001b[38;5;241m=\u001b[39mencoding_size)\n\u001b[0;32m      5\u001b[0m vsn_model \u001b[38;5;241m=\u001b[39m VSN(sequence_length, hidden_dim, encoding_size\u001b[38;5;241m=\u001b[39mencoding_size)\n\u001b[1;32m----> 6\u001b[0m x_ \u001b[38;5;241m=\u001b[39m \u001b[43mvsn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[2], line 92\u001b[0m, in \u001b[0;36mVSN.call\u001b[1;34m(self, x_t, s)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_t, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     83\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"前向传播方法。\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;124;03m        tf.Tensor: 输出张量的维度与隐藏层相等\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m     batch_size, time_steps, seq_len \u001b[38;5;241m=\u001b[39m x_t\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     93\u001b[0m     ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x_t, s)\n\u001b[0;32m     94\u001b[0m     w_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(ffn_output)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling VSN.call().\n\n\u001b[1mnot enough values to unpack (expected 3, got 2)\u001b[0m\n\nArguments received by VSN.call():\n  • x_t=tf.Tensor(shape=(2, 4), dtype=float32)\n  • s=tf.Tensor(shape=(2, 17), dtype=float32)"
     ]
    }
   ],
   "source": [
    "x = tf.random.uniform((time_steps, sequence_length))  # 示例输入 h_t\n",
    "s_indices = tf.zeros((time_steps,), dtype=tf.int32)  # 全部为0\n",
    "s = tf.one_hot(s_indices + 6, depth=encoding_size)\n",
    "\n",
    "vsn_model = VSN(sequence_length, hidden_dim, encoding_size=encoding_size)\n",
    "x_ = vsn_model(x, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtrend-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
