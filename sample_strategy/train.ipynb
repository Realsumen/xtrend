{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import importlib\n",
    "import os\n",
    "import loss_functions\n",
    "import dataprocessor as dataprocessor\n",
    "import model\n",
    "import pickle\n",
    "import change_point_detection\n",
    "from tqdm import tqdm\n",
    "from heartrate import trace\n",
    "\n",
    "def reload_custom_libs():\n",
    "    importlib.reload(loss_functions)\n",
    "    importlib.reload(dataprocessor)\n",
    "    importlib.reload(change_point_detection)\n",
    "    importlib.reload(model)\n",
    "\n",
    "\n",
    "\n",
    "reload_custom_libs()\n",
    "from change_point_detection import *\n",
    "from loss_functions import *\n",
    "from model import *\n",
    "from dataprocessor import *\n",
    "\n",
    "macd_timescales = [(8, 24), (16, 28), (32, 96)]\n",
    "rtn_timescales = [1, 21, 63, 126, 252]\n",
    "timesteps = 126\n",
    "folder_path = \"data\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得高斯变点片段\n",
    "# test = (20080101, 20090101)\n",
    "gaussion_process_list = []\n",
    "# for i in tqdm(range(15)):\n",
    "#     first_date, last_date = (test[0] + 10000 * i, test[1] + 10000 * i)\n",
    "#     # 处理数据\n",
    "#     data_list = process_data_list(files, macd_timescales, rtn_timescales, first_date=first_date, last_date=last_date)\n",
    "#     print(f\"{first_date}_{last_date}数据处理完成\")\n",
    "#     # 获得断点分割片段数据\n",
    "#     try:\n",
    "#         gaussion_process_list = get_segment_list(data_list=data_list)\n",
    "#     except:\n",
    "#         continue\n",
    "#     with open(f'segments/{first_date}_{last_date}.pkl', 'wb') as f:\n",
    "#         pickle.dump(gaussion_process_list, f)\n",
    "#     # # 读取数据\n",
    "#     # with open(f'segments/{first_date}_{last_date}.pkl', 'rb') as f:\n",
    "#     #     file = pickle.load(f)\n",
    "#     #     gaussion_process_list.extend(file)\n",
    "\n",
    "\n",
    "data_list = process_data_list(files, macd_timescales, rtn_timescales, last_date=20211231)\n",
    "\n",
    "pkl_files = [f for f in os.listdir(\"segments\") if f.endswith(\".pkl\")]\n",
    "for file in pkl_files:\n",
    "    with open(\"segments/\" + file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        gaussion_process_list.extend(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成张量, 并对类别信息进行one-hot 编码: 100%|██████████| 10/10 [00:06<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot 编码中...\n"
     ]
    }
   ],
   "source": [
    "# 生成数据： target_set 和 context_set\n",
    "asset_num, context_num = 10, 20\n",
    "target_set, labels, map = generate_tensors(data_list, time_steps=timesteps, encoder_type = \"one-hot\", return_map=True)\n",
    "with open(f'map.pkl', 'wb') as f:\n",
    "    pickle.dump(map, f)\n",
    "with open(f'map.pkl', 'rb') as f:\n",
    "    map = pickle.load(f)\n",
    "target_set, context_set, labels = gaussian_data_binder(\n",
    "    data_list,\n",
    "    target_set,\n",
    "    labels,\n",
    "    map=map,\n",
    "    asset_num=asset_num,\n",
    "    context_num=context_num,\n",
    "    gaussion_process_list=gaussion_process_list,\n",
    ")\n",
    "\n",
    "# 设置参数\n",
    "target_std = tf.cast(5e-2, tf.float64)\n",
    "hidden_dim = 64  # 128\n",
    "warm_up = 63\n",
    "features_len = len(macd_timescales) + len(rtn_timescales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_custom_libs()\n",
    "from change_point_detection import *\n",
    "from loss_functions import *\n",
    "from model import *\n",
    "from dataprocessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare 数据, 初始化数据集\n",
    "x, s = target_set[0], target_set[-1]\n",
    "x_c_rtn, x_c, s_c = context_set[0], context_set[0][:, :, :, 1:], context_set[-1]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_c, x_c_rtn, s_c, x, s, labels))\n",
    "\n",
    "timesteps = x.shape[-2]\n",
    "features_len = x.shape[-1]\n",
    "encoding_size = s.shape[-1]\n",
    "x_shape = (None, asset_num, timesteps, features_len)\n",
    "s_shape = (None, asset_num, timesteps, encoding_size)\n",
    "\n",
    "# 初始化模型\n",
    "xtrend_model = ModelWrapper(features_len, hidden_dim, encoding_size, num_heads=4, dropout_rate=0.4)\n",
    "xtrend_model.build((x_shape, s_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 训练模型脚本 depreciated\n",
    "# def train(model, dataset: tf.data.Dataset, batch_num: int, num_epochs: int, alpha: float):\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "#     dataset = dataset.shuffle(buffer_size=10000).batch(batch_num)\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         iter_count = 0\n",
    "#         for batch_data in tqdm(dataset, desc=f\"训练中...epoch{epoch}\"):\n",
    "#             # 对每一个批次进行处理\n",
    "#             x_c, x_c_rtn, s_c, x, s, labels = batch_data\n",
    "#             with tf.GradientTape() as tape:\n",
    "#                 result = model(x_c, x_c_rtn, s_c, x, s)\n",
    "#                 joint_loss, mle, sharpe = joint_loss_function(\n",
    "#                     result, labels, target_std, warm_up, alpha=alpha\n",
    "#                 )\n",
    "#             grads = tape.gradient(joint_loss, model.trainable_variables)\n",
    "#             optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "#             iter_count += 1\n",
    "#             if iter_count % 1 == 0 and True:\n",
    "#                 print(\n",
    "#                     f\"Epoch {epoch+1}/{num_epochs}, Iteration {iter_count}, Loss: {joint_loss.numpy()}, MLE: {mle.numpy()}, Sharpe: {sharpe.numpy()}\"\n",
    "#                 )\n",
    "\n",
    "#         print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {joint_loss.numpy()}\")\n",
    "\n",
    "# # 初始化模型\n",
    "# model = ModelWrapper(features_len, hidden_dim, encoding_size, num_heads=4)\n",
    "# model.build((x_shape, s_shape))\n",
    "# result = train(model, dataset, batch_num=64, num_epochs=100, alpha = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, dataset: tf.data.Dataset, batch_num: int, num_epochs: int, alpha: float, validation_split: float = 0.2, early_stopping_patience: int = 10):\n",
    "    # 将数据集拆分为训练集和验证集\n",
    "    dataset = dataset.shuffle(buffer_size=10000)\n",
    "    total_size = len(dataset)\n",
    "    val_size = int(total_size * validation_split)\n",
    "    train_size = total_size - val_size\n",
    "\n",
    "    train_dataset = dataset.take(train_size)\n",
    "    val_dataset = dataset.skip(train_size).batch(batch_num)\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3, clipnorm=100.0)\n",
    "    best_val_loss = np.inf\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 每个 epoch 开始的时候重新shuffle数据集\n",
    "        train_dataset = dataset.take(train_size).shuffle(buffer_size=10000).batch(batch_num)\n",
    "        for batch_data in tqdm(train_dataset, desc=f\"训练中...epoch{epoch}\"):\n",
    "            # 对每一个批次进行处理\n",
    "            x_c, x_c_rtn, s_c, x, s, labels = batch_data\n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model(x_c, x_c_rtn, s_c, x, s)\n",
    "                joint_loss, mle, sharpe = joint_loss_function(\n",
    "                    result, labels, target_std, warm_up, alpha=alpha\n",
    "                )\n",
    "            grads = tape.gradient(joint_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # 在每个epoch结束后，计算验证集的joint loss\n",
    "        val_joint_loss = 0.0\n",
    "        val_mle = 0.0\n",
    "        val_sharpe = 0.0\n",
    "        val_steps = 0\n",
    "        for val_batch_data in val_dataset:\n",
    "            x_c, x_c_rtn, s_c, x, s, labels = val_batch_data\n",
    "            result = model(x_c, x_c_rtn, s_c, x, s)\n",
    "            joint_loss, mle, sharpe = joint_loss_function(\n",
    "                result, labels, target_std, warm_up, alpha=alpha\n",
    "            )\n",
    "            val_joint_loss += joint_loss.numpy()\n",
    "            val_mle += mle.numpy()\n",
    "            val_sharpe += sharpe.numpy()\n",
    "            val_steps += 1\n",
    "\n",
    "        val_joint_loss /= val_steps\n",
    "        val_mle /= val_steps\n",
    "        val_sharpe /= val_steps\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Joint Loss: {val_joint_loss:.2f}, Validation MLE: {val_mle:.2f}, Validation Sharpe: {val_sharpe:.2f}\")\n",
    "        \n",
    "        # 早停检查\n",
    "        if val_joint_loss < best_val_loss:\n",
    "            best_val_loss = val_joint_loss\n",
    "            patience_counter = 0\n",
    "            model.save(f\"model/{epoch}_loss_{val_joint_loss:.2f}.keras\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch0:   0%|          | 0/31 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch0: 100%|██████████| 31/31 [03:42<00:00,  7.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500, Validation Joint Loss: 0.15, Validation MLE: 0.06, Validation Sharpe: 0.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch1: 100%|██████████| 31/31 [03:41<00:00,  7.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/500, Validation Joint Loss: 0.04, Validation MLE: 0.06, Validation Sharpe: -0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch2: 100%|██████████| 31/31 [03:50<00:00,  7.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/500, Validation Joint Loss: 0.27, Validation MLE: 0.06, Validation Sharpe: 0.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch3: 100%|██████████| 31/31 [03:54<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/500, Validation Joint Loss: 0.21, Validation MLE: 0.06, Validation Sharpe: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch4: 100%|██████████| 31/31 [03:54<00:00,  7.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/500, Validation Joint Loss: 0.23, Validation MLE: 0.06, Validation Sharpe: 0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch5: 100%|██████████| 31/31 [03:55<00:00,  7.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/500, Validation Joint Loss: 0.26, Validation MLE: 0.06, Validation Sharpe: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch6: 100%|██████████| 31/31 [05:28<00:00, 10.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/500, Validation Joint Loss: 0.43, Validation MLE: 0.06, Validation Sharpe: 0.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch7: 100%|██████████| 31/31 [04:22<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/500, Validation Joint Loss: 0.26, Validation MLE: 0.06, Validation Sharpe: 0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch8: 100%|██████████| 31/31 [04:07<00:00,  7.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/500, Validation Joint Loss: 0.38, Validation MLE: 0.06, Validation Sharpe: 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch9: 100%|██████████| 31/31 [06:23<00:00, 12.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500, Validation Joint Loss: 0.20, Validation MLE: 0.06, Validation Sharpe: 0.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch10: 100%|██████████| 31/31 [06:56<00:00, 13.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/500, Validation Joint Loss: 0.05, Validation MLE: 0.06, Validation Sharpe: -0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch11: 100%|██████████| 31/31 [07:00<00:00, 13.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/500, Validation Joint Loss: 0.59, Validation MLE: 0.06, Validation Sharpe: 0.53\n",
      "Early stopping at epoch 12\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "train(xtrend_model, dataset, batch_num=64, num_epochs=500, alpha=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'model' has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'model' has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "model.save(f\"final_model.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtrend-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
