{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.logging.TaskLevelStatusMessage is deprecated. Please use tf.compat.v1.logging.TaskLevelStatusMessage instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\tensorflow_probability\\python\\internal\\backend\\numpy\\_utils.py:48: The name tf.control_flow_v2_enabled is deprecated. Please use tf.compat.v1.control_flow_v2_enabled instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import importlib\n",
    "import os\n",
    "import loss_functions\n",
    "import dataprocessor\n",
    "import model\n",
    "import pickle\n",
    "import change_point_detection\n",
    "from tqdm import tqdm\n",
    "from heartrate import trace\n",
    "\n",
    "def reload_custom_libs():\n",
    "    importlib.reload(loss_functions)\n",
    "    importlib.reload(dataprocessor)\n",
    "    importlib.reload(change_point_detection)\n",
    "    importlib.reload(model)\n",
    "reload_custom_libs()\n",
    "from change_point_detection import *\n",
    "from loss_functions import *\n",
    "from model import *\n",
    "from dataprocessor import *\n",
    "\n",
    "macd_timescales = [(8, 24), (16, 28), (32, 96)]\n",
    "rtn_timescales = [1, 21, 63, 126, 252]\n",
    "timesteps = 126\n",
    "folder_path = \"data\"\n",
    "files = [f for f in os.listdir(folder_path) if f.endswith(\".xlsx\")]\n",
    "异常数据元组 = (\"CC00.NYB.xlsx\", \"LB00.CME.xlsx\", \"ES00.CME.xlsx\", \"NQ00.CME.xlsx\", \"YM00.CBT.xlsx\", \"SP00.CME.xlsx\")\n",
    "files = [file for file in files if file not in 异常数据元组]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15/15 [00:00<00:00, 56.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# 获得高斯变点片段\n",
    "test = [0, 200]\n",
    "\n",
    "for i in tqdm(range(15)):\n",
    "    lines = [test[0] + 200 * i, test[1] + 200 * i]\n",
    "    # if i < 15: continue\n",
    "    # # 处理数据\n",
    "    # data_list = process_data_list(files, macd_timescales, rtn_timescales, test=lines)\n",
    "    # print(f\"{lines[0]}_{lines[1]}数据处理完成\")\n",
    "    # print([len(data) for data in data_list])\n",
    "    # # 获得断点分割片段数据\n",
    "    # gaussion_process_list = get_segment_list(data_list=data_list)\n",
    "    # with open(f'{lines[0]}_{lines[1]}.pkl', 'wb') as f:\n",
    "    #     pickle.dump(gaussion_process_list, f)\n",
    "    with open(f'segments\\{lines[0]}_{lines[1]}.pkl', 'rb') as f:\n",
    "        gaussion_process_list = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获得全部数据\n",
    "data_list = process_data_list(files, macd_timescales, rtn_timescales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_custom_libs()    \n",
    "from change_point_detection import *\n",
    "from loss_functions import *\n",
    "from model import *\n",
    "from dataprocessor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成张量, 并对类别信息进行one-hot 编码:   0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "生成张量, 并对类别信息进行one-hot 编码: 100%|██████████| 11/11 [00:41<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one-hot 编码中...\n"
     ]
    }
   ],
   "source": [
    "# 生成 target_set 和 context_set\n",
    "asset_num, context_num = len(data_list), 2\n",
    "target_set, labels, map = generate_tensors(data_list, time_steps=timesteps, encoder_type = \"one-hot\", return_map=True)\n",
    "target_set, context_set, labels = gaussian_data_binder(\n",
    "    data_list,\n",
    "    target_set,\n",
    "    labels,\n",
    "    map=map,\n",
    "    asset_num=2,\n",
    "    context_num=2,\n",
    "    gaussion_process_list=gaussion_process_list,\n",
    ")\n",
    "\n",
    "# 设置参数\n",
    "target_std = tf.cast(5e-2, tf.float64)\n",
    "hidden_dim = 64  # 128\n",
    "warm_up = 63\n",
    "features_len = len(macd_timescales) + len(rtn_timescales)\n",
    "\n",
    "# declare 数据, 初始化数据集\n",
    "x, s = target_set[0], target_set[-1]\n",
    "x_c_rtn, x_c, s_c = context_set[0], context_set[0][:, :, :, 1:], context_set[-1]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((x_c, x_c_rtn, s_c, x, s, labels))\n",
    "\n",
    "timesteps = x.shape[-2]\n",
    "features_len = x.shape[-1]\n",
    "encoding_size = s.shape[-1]\n",
    "x_shape = (None, asset_num, timesteps, features_len)\n",
    "s_shape = (None, asset_num, timesteps, encoding_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "def train(model, dataset: tf.data.Dataset, batch_num: int, num_epochs: int, alpha: float):\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    dataset = dataset.shuffle(buffer_size=10000).batch(batch_num)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        iter_count = 0\n",
    "        for batch_data in tqdm(dataset, desc=f\"训练中...epoch{epoch}\"):\n",
    "            # 对每一个批次进行处理\n",
    "            x_c, x_c_rtn, s_c, x, s, labels = batch_data\n",
    "            with tf.GradientTape() as tape:\n",
    "                result = model(x_c, x_c_rtn, s_c, x, s)\n",
    "                joint_loss, mle, sharpe = joint_loss_function(\n",
    "                    result, labels, target_std, warm_up, alpha=alpha\n",
    "                )\n",
    "            grads = tape.gradient(joint_loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            iter_count += 1\n",
    "            if iter_count % 1 == 0 and False:\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{num_epochs}, Iteration {iter_count}, Loss: {joint_loss.numpy()}, MLE: {mle.numpy()}, Sharpe: {sharpe.numpy()}\"\n",
    "                )\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {joint_loss.numpy()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练中...epoch0:   0%|          | 0/14 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m ModelWrapper(features_len, hidden_dim, encoding_size, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# model.build((x_shape, s_shape))\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataset, batch_num, num_epochs, alpha)\u001b[0m\n\u001b[0;32m     11\u001b[0m x_c, x_c_rtn, s_c, x, s, labels \u001b[38;5;241m=\u001b[39m batch_data\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m---> 13\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_c_rtn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_c\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     joint_loss, mle, sharpe \u001b[38;5;241m=\u001b[39m joint_loss_function(\n\u001b[0;32m     15\u001b[0m         result, labels, target_std, warm_up, alpha\u001b[38;5;241m=\u001b[39malpha\n\u001b[0;32m     16\u001b[0m     )\n\u001b[0;32m     17\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(joint_loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32mc:\\Users\\jrfundoffice02\\anaconda3\\envs\\xtrend-env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\jrfundoffice02\\workspace\\xtrend\\model.py:421\u001b[0m, in \u001b[0;36mModelWrapper.build\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_shape):\n\u001b[0;32m    415\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[38;5;124;03m    构建层的权重。\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m        input_shape (tuple): 输入的形状。\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 421\u001b[0m     x_shape, s_shape \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    422\u001b[0m     inter_dimensions \u001b[38;5;241m=\u001b[39m (x_shape[\u001b[38;5;241m0\u001b[39m], x_shape[\u001b[38;5;241m1\u001b[39m], x_shape[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_dim)\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV_encoder\u001b[38;5;241m.\u001b[39mbuild(\n\u001b[0;32m    424\u001b[0m         ((x_shape[\u001b[38;5;241m0\u001b[39m], x_shape[\u001b[38;5;241m1\u001b[39m], x_shape[\u001b[38;5;241m2\u001b[39m], x_shape[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), s_shape)\n\u001b[0;32m    425\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = ModelWrapper(features_len, hidden_dim, encoding_size, num_heads=4)\n",
    "model.build((x_shape, s_shape))\n",
    "result = train(model, dataset, batch_num=64, num_epochs=100, alpha = 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.9790909090909095"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-32.77 / 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xtrend-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
